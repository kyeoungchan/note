# 🧑🏻‍💻 카프카 실전 프로젝트
- [요구 사항](#-요구-사항)
- [적재 정책](#-적재-정책)
- [데이터 포맷](#-데이터-포맷)
- [프로듀서](#-프로듀서)
- [토픽](#-토픽)
- [아키텍처](#-아키텍처)
- [프로젝트 링크](#-프로젝트-링크)


웹 페이지의 사용자 이벤트 수집은 서비스에 영향을 미치지 않으면서도 안정적으로 유지되어야 한다.  
이벤트 수집 파이프라인과 서비스의 커플링을 최소화하고 확장성 높은 파이프라인을 만드는 것이 중요하다.  
➡ 카프카는 이러한 웹 이벤트 수집 파이프라인을 만들고 운영하기에 최적화되어 있다.

카프카는 예상치 못한 데이터의 급격한 증가가 발생하더라도 안정적으로 운영하는 데에 강점이 있다.  
웹 페이지의 사용자 이벤트가 상당히 많이 발생하더라도 발생한 이벤트들은 모두 카프카의 토픽에 쌓이기 때문이다.  
일단 토픽에 쌓이면 그 다음은 컨슈머가 자신이 할 수 있는 양만큼 최종 적재 애플리케이션에 저장하면 된다.

### ✅️ 요구 사항
이름을 입력하고 자신이 좋아하는 색상을 고르는 버튼을 누르면 해당 이벤트와 유저 에이전트 정보를 카프카 토픽으로 전달하고, 최종적으로 하둡과 엘라스틱서치에 적재되는 것을 목표로 한다.
- 하둡
    - 대용량 데이터를 분석 처리할 수 있다.
    - HDFS: 대용량 파일을 하둡에 안정적으로 저장할 수 있게 하는 파일 시스템
    - 사용자의 이벤트를 HDFS에 적재하면 사용자 이벤트가 기하급수적으로 늘어나는 상황에서도 안정적으로 적재하고 분석할 수 있다.
- 엘라스틱서치
    - 아파치 루씬(Apache Lucene) 기반 오픈소스 분산 검색 엔진이다.
    - 엘라스틱서치에 데이터를 쌓아서 방대한 양의 데이터를 저장, 검색, 분석할 수 있다.
    - 사용자의 이벤트를 엘라스틱서치에 쌓아서 키바나를 통해 데이터를 시각화하고 분석할 수 있다.
    - 자세한 내용은 [엘라스틱서치 페이지](https://github.com/kyeoungchan/note/tree/main/elastic-stack/elasticsearch)를 참고하자.

<br>

### ✅️ 적재 정책
적재 파이프라인을 만들 때 가장 처음 결정해야하는 것은 적재 정책이다.  
파이프라인의 운영 난이도는 정책에 따라 달라진다.
- 일부 데이터가 유실되거나 중복 적재되어도 무관한 정책 ➡ 파이프라인의 운영 난이도는 낮아진다.
- 데이터가 유실, 중복 없이 정확히 한 번(exactly once) 적재되는 정책 ➡ 파이프라인 운영 난이도는 상당히 올라간다.

0.11.0.0 이후 버전부터는 멱등성 프로듀서(idempotence produce)를 통해 정확히 한 번 전달을 지원한다.
- 여기서의 '전달': 프로듀서부터 브로커까지 전달되는 것. 적재와 별개다.
- 여기서의 '적재': 프로듀서부터 컨슈머를 넘어 최종적으로 하둡이나 엘라스틱서치까지 데이터가 저장되는 것

정확히 한 번 전달되더라도 정확히 한 번 적재는 되지 않을 수 있다.  
컨슈머의 커밋 시점과 데이터 적재가 동일 트랜잭션에서 처리되어야 정확히 한 번 적재될 수 있기 때문이다.

<br>

HDFS 적재, S3 적재 컨슈머 애플리케이션에서는 컨슈머의 커밋과 저장이 동일 트랜잭션으로 처리하는 것이 불가능하다.  
➡ HDFS 적재, S3 적재 컨슈머 애플리케이션은 이슈 발생 시점을 확인하고 특정 파티션의 특정 오프셋부터 다시 적재할 수밖에 없다.

<br>

정확히 한 번 적재가 필요할 경우 ➡ 멱등성 프로듀서를 사용하고 고유한 키를 지원하는 데이터베이스 시스템에 적재하는 것이 가장 확실하다.  
예를 들어, MySQL에 고유 키(unique key)를 가진 테이블을 생성하고 해당 테이블에 insert하는 적재 파이프라인을 만드는 경우, 컨슈머가 중복해서 insert를 시도하더라도 이미 고유 키로 적재된 데이터가 존재하므로 중복 적재되지 않게 된다.

<br>

웹 페이지의 사용자 이벤트는 사용자가 사용하는 환경(ex. 산속)에 따라 유실이 발생할 가능성이 크다.  
그리고, 최종 적재하고자 하는 HDFS 적재의 경우 트랜잭션이 지원되지 않으므로 컨슈머의 장애가 발생했을 때 데이터의 중복이 발생할 여지가 있다.  
따라서 정확히 한 번 적재 정책을 가져가는 것이 어렵기 때문에 중복도 허용할 수 있다.

<br>

파이프라인 정책
- 일부 데이터의 유실 또는 중복 허용
- 안정적으로 끊임없는 적재
- 갑작스럽게 발생하는 많은 데이터양을 허용

<br>

### ✅ 데이터 포맷
데이터 파이프라인에서 데이터를 담는 용도로 사용되는 데이터 포맷은 매우 다양한 선택지가 있다.  
우선, VO(Value Object) 형태로 객체를 선언하여 직렬화하여 전송하는 방법을 생각해볼 수 있다.

단점
- 프로듀서와 컨슈머에서 동일한 버전의 VO 객체를 선언해서 사용해야한다는 문제점이 있다.
- 스키마가 변경될 경우(ex. 변수가 추가되거나 삭제되는 경우) 프로듀서와 컨슈머 둘 다 소스코드 업데이트가 필요하므로 비용이 크다.
- 직렬화된 객체는 `kafka-console-consumer` 명령어를 통해 출력할 경우 내부 데이터를 확인할 수 없기 때문에 디버깅이 어려우며, 디버깅을 위해서는 해당 객체에 특화된 역직렬화 클래스가 필요하다.

<br>

데이터 포맷을 선택할 때 우선적으로 생각해볼 2가지가 있는데, 스키마의 변화 유연성과 명령어를 통한 디버깅의 편리성이다.  
➡ 2가지 모두 충족되는 데이터 포맷이 JSON(JavaScript Object Notation)이다.
- JSON은 String으로 선언되어 토픽에 String 또는 ByteArray로 직렬화하여 전송하고, `kafka-console-consumer` 명령어를 통해 데이터를 출력할 수 있다.
- 키-값 쌍으로 이루어진 구조를 가지고 있으므로 스키마의 변경에 유연하게 대처할 수 있다.
- HDFS에 JSON 포맷으로 파일을 적재하면 아파치 하이브(Apache Hive)로 external table을 생성하여 SQL을 사용해서 필요한 정보를 쿼리로 뽑아낼 수 있다.
- elasticsearch는 JSON 포맷 기반으로 문서 파일을 저장하기 때문에 별다른 포맷 변경 없이 데이터를 적재할 수 있다.

<br>

### ✅ 프로듀서
웹 페이지에서 생성된 이벤트를 받는 REST API 클라이언트를 만들고 전달받은 이벤트를 가공하여 토픽으로 전달하는 역할을 한다.  
RestController로 받은 데이터를 토픽으로 전달할 때는 스프링 카프카 라이브러리를 사용한다.  
스프링 카프카의 `KafkaTemplate`으로 프로듀서를 구현하여 데이터를 전송한다.

<br>

**`acks` 설정**  
프로듀서를 운영할 때 가장 처음 고민해야 할 옵션은 `ack`를 어떤 값으로 설정할지다.
- `all`: 클러스터 또는 네트워크에 이상이 생겼을 경우 복구할 확률이 가장 높지만 그만큼 프로듀서가 클러스터로 데이터를 저장하는 데에 시간이 오래걸린다.
- 1 또는 0: 속도는 빠르지만 클러스터에 이상이 생겼을 경우 데이터를 복구하지 못하고 유실이 발생할 수 있다.

이번 애플리케이션에서는 데이터 전송에 일부 유실이나 중복이 발생하더라도 안정적이고 빠른 파이프라인을 구성하는 것이 목표이므로 `acks`를 1로 설정한다.

<br>

**`min.insync.replicas` 설정**  
`acks`를 1로 설정한 경우 `min.insync.replicas` 설정을 무시하고 리더 파티션에 지속 적재하므로 따로 설정할 필요가 없다.  
➡ `acks`를 `all`로 설정했을 때만 `min.insync.replicas` 설정이 유요하다.  
참고: [카프카 프로듀서](https://github.com/kyeoungchan/note/tree/main/kafka/detail)

<br>

**파티셔너 설정**  
파티셔너를 활용하면 메시지 키 또는 메시지 값을 기반으로 어떤 파티션으로 전달될지 결정하는 로직을 적용할 수 있다.  
그러나 웹 페이지에서 생성된 데이터는 특별히 파티션을 분류할 필요가 없기 때문에 기본 파티셔너인 `UniformStickyPartitioner`를 사용한다.

<br>

**재시도(retries) 설정**  
클러스터 또는 네트워크 이슈로 인해 데이터가 정상적으로 전송되지 않았을 때 프로듀서는 다시 전송을 시도한다.  
프로듀서가 전송을 재시도할 경우 토픽으로 전송된 데이터의 중복이 발생할 수 있고, 전송 시점의 역전으로 인해 전송 순서와 토픽에 적재된 데이터의 순서가 바뀔 수 있다.  
여기서는 토픽의 데이터 순서를 지키지 않고 데이터의 중복을 허용하기 때문에 별도로 설정하지 않는다.

<br>

**프로듀서의 압축 옵션 설정**  
프로듀서의 압축은 `gzip`, `sanpy`, `lz4`, `zstd` 중 한 개를 고를 수 있다.  
압축을 하면 클러스터에 적재되는 데이터의 총 용량을 줄이고 네트워크의 사용량을 줄이는 데 효과적이다.  
그러나 프로듀서와 컨슈머에서 데이터를 사용할 때 CPU와 메모리 사용량이 늘어나고 압축을 하지 않았을 때보다 처리량이 줄어들 수 있다.  
여기서는 압축을 하지 않는다.

<br>

### ✅ 토픽
**파티션 개수**  
토픽을 설정할 때 가장 처음 고민하는 것은 파티션 개수다.  
파티션 개수는 데이터 처리 순서를 지켜야하는지 여부에 따라 엄격하게 정할지 말지 결정한다.  
하둡 또는 엘라스틱서치에 데이터를 순서대로 적재해야 한다면 파티션 개수를 엄격하게 정해야 한다.

그러나 데이터를 순서대로 적재하지 않더라도 하둡과 엘라스틱서치에서 데이터를 조회할 때 시간 순서대로 조회할 수 있다.  
➡ 이벤트가 발생할 때 이벤트 발생 시간을 데이터에 같이 조합해서 보내는 것이다.  
➡ 적재되는 순서가 이벤트 발생 순서와 달라도 되기 때문에 ➡ 파티션을 여러 개로 생성하여 병렬로 컨슈머를 운영해도 된다.  
그러므로 토픽의 파티션 개수는 2개 이상으로 설정한다.

<br>

**메시지 키의 사용 여부**  
메시지 키를 사용하고 커스텀 프로듀서 파티셔너를 사용하지 않을 경우 메시지 키의 해시값으로 파티션이 분배된다.  
➡ 추후 파티션이 증가되면 해시값과 파티션의 매칭이 깨지기 때문에 메시지 키를 활용하고 특정 파티션에 할당하는 컨슈머를 운영할 경우 매우 곤란해진다.  
여기서는 메시지 키를 사용하지 않고, 토픽에 들어오는 데이터의 양에 따라 파티션 개수를 가변적으로 설정할 수 있게 한다.

<br>

**복제 개수(replication factor)**  
복제 개수가 높으면 높을수록 데이터의 복구 확률이 높아진다.  
다만, 복제 개수가 너무 높으면 팔로워 파티션이 데이터를 복제하는 데에 시간이 오래 걸릴 수 있으며, 클러스터 전체를 봤을 때 저장되는 데이터의 용량도 그만큼 더 늘어난다.  
여기서는 클러스터 브로커 1대에 이슈가 발생했을 경우에도 안정적으로 데이터를 받기 위한 최소 설정으로 2를 설정한다.

<br>

### ✅ 컨슈머
토픽에 저장되어 있는 웹 이벤드를 하둡과 엘라스틱서치에 저장하는 로직을 만드는 두 가지 방법
1. 컨슈머 API를 사용하여 직접 애플리케이션을 개발하는 방법
    - 컨슈머를 직관적으로 운영하는 가장 좋은 방법이다.
    - Java 기반 애플리케이션으로 직접 개발할 경우 타깃 애플리케이션과 연동하는 라이브러리를 직접 선택, 적용할 수 있다.
    - 컨슈머 내부 로직에 타깃 애플리케이션과 연동 시 최적화할 수 있는 옵션을 거의 제한 없이 적용할 수 있기 때문에 개발 허용 범위가 넓다.
    - 반복적으로 생성되는 파이프라인 운영이 필요할 때 확장 가능한 멀티 스레드 애플리케이션을 개발하기 위해서는 상당한 노력이 들어간다.
2. 커넥트를 사용하는 방법(참고: [카프카 커넥트](https://github.com/kyeoungchan/note/tree/main/kafka/kafkaconnect))
    - 분산 커넥트를 사용하면 REST API를 통해 커넥터로 반복적인 파이프라인을 쉽게 생성할 수 있다.
    - 커넥트를 사용할 때 오픈소스로 공개된 커텍터를 사용할 수도 있고 직접 커넥터를 개발하여 커넥트에 적용하여 사용할 수 있다.
    - 그러나 오픈소스 커넥터들은 각기 다른 라이선스를 가지고 있으며 법적인 문제로 오픈소스를 사용할 수 없을 수도 있으므로 주의해야 한다.
    - 커넥터를 직접 개발하면 법적인 문제에서 자유로울 수 있다.
        - 커넥터를 직접 개발할 경우 HDFS와 elasticsearch에 적재하기 위해서는 싱크 커넥터를 구현해야 한다.
        - 카프카 connect-api 라이브러리에 포함된 `SinkConnector`와 `SinkTask` 추상클래스를 상속받으면서 커스텀 싱크 커넥터를 개발할 수 있다.

개발하는 환경에 따라 선택하면 된다.  
이미 상용 인프라에 분산 커넥트가 구축되어 있다면 싱크 커넥터를 활용하는 것이 좋다.  
분산 커넥트가 없다면 구축하는 데에 시간과 인프라 비용이 발생할 수 있으므로 컨슈머 API를 활용하여 멀티 스레드 Java 애플리케이션을 만들고 배포하여 운영하는 것이 좋다.

여기서는 둘다 해본다.
- HDFS 적재 ➡ 컨슈머 API를 사용하여 만든 멀티 스레드 JAVA 애플리케이션으로 개발
- elasticsearch ➡ 커스텀 싱크 커넥터를 개발하여 커넥트에 적용


<br>

### ✅ 아키텍처
![web_page_event_load_pipeline_architecture.jpeg](../res/web_page_event_load_pipeline_architecture.jpeg)  
아키텍처를 구현하기 위해 필요한 작업 리스트
- 로컬 하둡, 엘라스틱서치, 키바나 설치
- 토픽 생성
- 이벤트 수집 웹 페이지 개발
- REST API 프로듀서 애플리케이션 개발
- 하둡 적재 컨슈머 애플리케이션 개발
- 엘라스틱서치 싱크 커넥터 개발

<br>

### ✅ 프로젝트 링크
- [favorite-color-webpage](https://github.com/kyeoungchan/favorite-color-webpage)
- [kafka-spring-producer-with-rest-controller](https://github.com/kyeoungchan/kafka-spring-producer-with-rest-controller)
- [kafka-multi-consumer-thread-hdfs-save](https://github.com/kyeoungchan/kafka-multi-consumer-thread-hdfs-save)
- [elasticsearch-kafka-consumer
  ](https://github.com/kyeoungchan/elasticsearch-kafka-consumer)

<br>

#### 🧑🏻‍💻 실행 순서
1. 하둡을 실행한다. (참고: [하둡 실행 가이드](https://github.com/kyeoungchan/note/tree/main/hadoop/settings))
2. 엘라스틱 서치를 실행한다. (참고: [엘라스틱서치 실행 가이드](https://github.com/kyeoungchan/note/tree/main/elastic-stack/settings))
3. [kafka-spring-producer-with-rest-controller](https://github.com/kyeoungchan/kafka-spring-producer-with-rest-controller)를 먼저 실행한다.
4. [kafka-multi-consumer-thread-hdfs-save](https://github.com/kyeoungchan/kafka-multi-consumer-thread-hdfs-save)를 실행한다.
5. [elasticsearch-kafka-consumer
   ](https://github.com/kyeoungchan/elasticsearch-kafka-consumer)에 적힌 가이드대로 실행한다.

<br>

웹 페이지(http://127.0.0.1:5500/index.html)를 접속하고 이름을 작성하고, 색성 버튼을 무작위로 클릭한다.  
우리는 하둡에 데이터를 저장할 때 최소 10개 이상의 데이터가 버퍼에 쌓였을 경우 HDFS에 저장하는 로직을 작성했고, 파티션은 3개로 설정했으므로 최소 30번 이상 웹 이벤트를 발생시켜야 각 파티션의 데이터가 HDFS에 쌓인다.  
➡ 30번 이상 색상 버튼을 클릭한 후 아래의 커맨드로 확인해본다.
```shell
$ hdfs dfs -ls -R /
2025-12-20 15:53:40,117 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - kyeongchanwoo supergroup          0 2025-12-20 15:53 /data
-rw-r--r--   3 kyeongchanwoo supergroup        215 2025-12-20 15:48 /data/color-0-0.log
-rw-r--r--   3 kyeongchanwoo supergroup       2174 2025-12-20 15:53 /data/color-0-1.log
-rw-r--r--   3 kyeongchanwoo supergroup       2176 2025-12-20 15:53 /data/color-0-11.log
-rw-r--r--   3 kyeongchanwoo supergroup       2176 2025-12-20 15:53 /data/color-0-21.log
```


<br>

**참고 자료**  
[아파치 카프카 애플리케이션 프로그래밍 with 자바](https://product.kyobobook.co.kr/detail/S000001842177)