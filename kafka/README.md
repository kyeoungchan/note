- [카프카](#-카프카)
- [카프카 실전 프로젝트](#-카프카-실전-프로젝트)
# 💻 카프카
- [빅데이터 파이프라인에서 카프카의 역할](#-빅데이터-파이프라인에서-카프카의-역할)
  - [높은 처리량](#-높은-처리량)
  - [확장성](#-확장성)
  - [영속성](#-영속성)
  - [고가용성](#-고가용성)
- [데이터 레이크 아키텍처와 카프카의 미래](#-데이터-레이크-아키텍처와-카프카의-미래)

> 링크드인의 데이터 팀이 데이터 파이프라인의 복잡도를 낮추기 위해 만든 신규 시스템이다.  
> 카프카는 기업의 대용량 데이터를 수집하고 이를 사용자들이 실시간 스트림으로 소비할 수 있게 만들어주는 일종의 중추 신경으로 동작한다.

기존에 1:1 매칭으로 개발하고 운영하던 데이터 파이프라인은 커플링으로 인해 한쪽의 이슈가 다른 한쪽의 애플리케이션에 영향을 미쳤다.  
➡ 카프카를 사용하면 소스 애플리케이션에서 생성되는 데이터를 카프카에 일단 넣으면 된다.  


![kafka_producer_consumer_topic.jpeg](res/kafka_producer_consumer_topic.jpeg)  
카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 방식의 큐 자료구조와 유사하다.  
프로듀서: 큐에 데이터를 보내는 쪽  
컨슈머: 큐에서 데이터를 가져가는 쪽  

<br>

상용 환경에서 카프카는 최소 3대 이상의 서버(브로커)에서 분산 운영하여 프로듀서를 통해 전송받은 데이터를 파일 시스템에 안전하게 보관한다.  
서버 3대 이상으로 이루어진 카프카 클러스터 중 일부 서버에 장애가 발생하더라도 데이터를 지속적으로 복제하기 때문에 안전하게 운영할 수 있다.  
또한, 데이터를 묶음 단위로 처리하는 배치 전송을 통해 낮은 지연과 높은 데이터 처리량도 가지게 되었다.  

<br>

[💻 카프카 깃허브 저장소](https://github.com/apache/kafka)에서 다운로드하여 내부 동작을 확인할 수 있다.  
기능을 추가하거나 수정하고 싶다면 [KIP(Kafka Improvement Proposal)](http://bit.ly/3aKK2FF)를 통해 카프카를 발전하는 데 기여할 수 있다.  

<br>

## 💡 빅데이터 파이프라인에서 카프카의 역할
**빅데이터**  
현대의 IT 서비스는 디지털 정보로 기록되는 모든 것을 저장한다.  
쇼핑몰의 결제 내역, 방문한 위치 정보, 소셜 사이트에 남긴 댓글 등  
실시간으로 저장하는 데이터의 양은 최소 테라바이트(Terabyte, TB)를 넘어서 엑사바이트(Exabyte, EB)를 웃돈다.  
빅데이터로 적재되는 데이터 종류는 기존 데이터베이스에서도 사용하기 편리한 스키마(schema) 기반의 정형 데이터부터 일정한 규격이나 형태를 지니지 않은 비정형 데이터까지 있다.  
그림, 영상, 음성과 같은 데이터는 비정형 데이터 중 일부라고 볼 수 있다.  

<br>

**데이터 레이크(data lake)**  
빅데이터를 저장하고 활용하기 위해 일단 생성되는 데이터를 모두 모으는 개념  
데이터 웨어하우스(data warehouse)와 다르게 필터링되거나 패키지화되지 않은 데이터가 저장된다는 점이 특징이다.  
데이터 레이크로 데이터를 모으는 과정에서 데이터를 추출하고 변경, 적재하는 과정을 묶은 데이터 파이프라인을 구축해야하는데, 데이터 파이프라인을 안정적이고 확장성 높게 운영하기 위한 좋은 방법 중 하나가 바로 아파키 카프카를 활용하는 것이다.  

<br>

### ✅ 높은 처리량
카프카는 프로듀서가 브로커로 데이터를 보낼 때와 컨슈머가 브로커로부터 데이터를 받을 때 모두 묶어서 전송한다.  
동일한 양의 데이터를 보낼 때 네트워크 통신 횟수를 최소한으로 줄인다면 동일 시간 내에 더 많은 데이터를 전송할 수 있다.  
많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그데이터를 처리하는 데에 적합하다.  
또한, 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬 처리할 수 있다.  
➡ 파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것이다.


<br>

### ✅ 확장성
데이터 파이프라인에서 데이터를 모을 때 데이터가 얼마나 들어올지는 예측하기 어렵다.  
카프카는 가변적인 환경에서 안정적으로 확장 가능하도록 설계되어 있다.
- 데이터가 적을 때는 카프카 클러스터의 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면 클러스터의 브로커 개수를 자연스럽게 늘려 스케일 아웃(scale-out)할 수 있다.  
- 데이터 개수가 적어지고 추가 서버들이 더는 필요없어지면 브로커 개수를 줄여 스케일 인(scale-in)할 수 있다.

➡ 카프카의 스케일 아웃, 스케일 인 과정은 클러스터의 무중단 운영을 지원하므로 365일 24시간 데이터를 처리해야 하는 커머스나 은행 같은 비즈니스 모델에서도 안정적인 운영이 가능하다.  

<br>

### ✅ 영속성
영속성이란 데이터를 생성한 프로그램이 종료되더라도 사라지지 않은 데이터의 특성을 뜻한다.  
카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장한다.  
디스크 기반의 파일 시스템을 활용한 덕분에 브로커 애플리케이션이 장애 발를 인해 급작스럽게 종료되더라도 프로세스를 재시작하여 안전하게 데이터를 다시 처리할 수 있다.
> **🤔 파일 시스템에 데이터 적재하고 사용하는 것은 느리지 않을까?**  
> 카프카는 운영체제 레벨에서 파일 시스템을 최대한 활용하는 방법을 적용하였다.  
> 운영체제에서는 파일 I/O 성능 향상을 위해 페이지 캐시(page cache) 영역을 메모리에 따로 생성하여 사용한다.  
> 페이지 캐시란, OS에서 파일 입출력의 성능 향상을 위해 만들어 놓은 메모리 영역을 뜻한다.  
> 페이지 캐시 메모리 영역을 사용하여 한 번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식이기 때문에 카프카가 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높은 것이다.  
> 만약 페이지 캐시를 사용하지 않았다면 카프카에서 캐시를 직접 구현 ➡ 지속적으로 변경되는 데이터 때문에 가비지 컬렉션이 자주 일어나 속도가 현저히 느려질 것이다.

<br>

### ✅ 고가용성
3개 이상의 서버들로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다.  
클러스터로 이루어진 카프카는 데이터의 복제(replication)를 통해 고가용성의 특징을 가지게 되었다.  
이에 더하여 서버를 직접 운영하는 온프레미스(on-premise) 환경의 서버 랙 또는 퍼블릭 클라우드(public cloud)의 리전 단위 장애에도 데이터를 안전하게 복제할 수 있는 브로커 옵션들이 준비되어 있다.  
> **🤔 카프카 클러스터를 3대 이상의 브로커들로 구성해야 하는 이유**  
> 카프카를 안전하게 운영하기 위해 최소 3대 이상의 브로커로 클러스터를 구성할 것을 추천한다.  
> 카프카 클러스터를 구축할 때 브로커 개수의 제한은 없다.  
> - 1대로 운영할 경우: 브로커의 장애는 서비스 장애로 이어지게 되므로 테스트 목적으로만 사용
> - 2대로 운영할 경우: 브로커 간에 데이터가 복제되는 시간 차이로 인해 일부 데이터가 유실될 가능성이 있다.
> 
> 
> 유실을 막기 위해서는 `min.insync.replicas` 옵션을 2로 설정하면 최소 2개 이상의 브로커에 데이터가 완전히 복제됨을 의미한다.  
> 이 옵션을 2로 사용할 때는 3개 중 1개의 브로커에 장애가 나더라도 지속적으로 데이터를 처리할 수 있게 하기 위해서 브로커를 3대 이상으로 운영해야 한다. ➡ `min.insync.replicas` 옵션값보다 작은 수의 브로커가 존재할 때는 토픽에 더는 데이터를 넣을 수가 없다.

<br>

높은 처리량, 확장성, 영속성, 고가용성 특징을 가진 카프카는 데이터 파이프라인을 안전하고 확장성 높게 운영할 수 있도록 설계되었다.  
여기에 추가적으로 카프카 주변 생태계를 지탱하는 애플리케이션들은 카프카를 데이터 파이프라인으로 더욱 빠르게 적용시킬 수 있도록 도와준다.  

![kafka_ecosystem_schema.jpeg](res/kafka_ecosystem_schema.jpeg)

<br>

## 💡 데이터 레이크 아키텍처와 카프카의 미래
데이터 레이크 아키텍처의 2가지 종류
1. 람다 아키텍처(lambda architecture)
   - 레거시 데이터 수집 플랫폼을 개선하기 위해 구성한 아키텍처다.
   > 초키 빅데이터 플랫폼은 엔드 투 엔드로 각 서비스 애플리케이션으로부터 데이터를 배치로 모았다.  
   ![legacy_data_platform_architecture.jpeg](res/legacy_data_platform_architecture.jpeg)  
   > - 데이터를 배치로 모으는 구조는 유연하지 못했으며 실시간으로 생성되는 데이터들에 대한 인사이트를 서비스 애플리케이션이 빠르게 전달하지 못하는 단점이 있다.
   > - 원천 데이터로부터 파생된 데이터의 히스토리를 파악하기 어려웠고, 계속되는 데이터의 가공으로 인해 데이터가 파편화되면서 데이터 거버넌스(data governance: 데이터 표준 및 정책)를 지키기 어려웠다.
   - 기존의 배치 데이터를 처리하는 부분 위에 스피드 레이어(speed layer)라고 불리는 실시간 데이터 ETL(extract, transform, load)작업 영역을 정의한 아키텍처가 바로 람다 아키텍처다.  
     ![lambda_architecture.jpeg](res/lambda_architecture.jpeg)  
     - 배치 레이어: 배치 데이터를 모아서 특정 시간, 타이밍마다 일괄 처리한다.
     - 서빙 레이어: 가공된 데이터를 데이터 사용자, 서비스 애플리케이션이 사용할 수 있도록 데이터가 저장된 공간
     - 스피드 레이어: 서비스에서 생성되는 원천 데이터를 실시간으로 분석하는 용도로 사용된다.
       - 배치 데이터에 비해 낮은 지연으로 분석이 필요한 경우 사용
       - 스피드 레이어에서 가공, 분석된 실시간 데이터는 사용자 또는 서비스에서 직접 사용할 수 있지만 필요한 경우에는 서빙 레이어로 데이터를 보내서 저장하고 사용할 수 있다.
       - 람다 아키텍처에서 카프카는 스피드 레이어에 위치한다.
   - 데이터를 배치 처리하는 레이어와 실시간 처리하는 레이어를 분리하여 레이어가 2개로 나뉘기 때문에 생기는 단점이 있다.
     - 데이터를 분석, 처리하는 데에 필요한 로직이 2벌로 각각의 레이어에 따로 존재해야 한다는 점
     - 배치 데이터와 실시간 데이터를 융합하여 처리할 때는 다소 유연하지 못한 파이프라인을 생성해야 한다는 점
2. 카파 아키텍처(kappa architecture)
   - 람다 아키텍처와 유사하지만 배치 레이어를 제거하고 모든 데이터를 스피드 레이어에 넣어서 처리한다는 점이 다르다.  
     ![kappa_architecture.jpeg](res/kappa_architecture.jpeg)  
     (위의 그림에서 배치 레이어가 아니라 스피드 레이어다. 오타다.)
   - 스피드 레이어에서 모든 데이터를 처리하므로 서비스에서 생성되는 모든 종류의 데이터를 스트림 처리해야 한다.

<br>

> 배치 데이터 vs 스트림 데이터  
> - 배치 데이터
>   - 초, 분, 시간, 일 등으로 한정된 기간 단위 데이터를 뜻한다.
>   - 배치 데이터를 일괄 처리(batch processing)하는 것이 특징이다.
>   - 인터넷 쇼핑몰에서 지난 1분 간 주문한 제품 목록, 2025년 신입생 목록
> - 스트림 데이터
>   - 한정되지 않은 데이터로 시작 데이터와 끝 데이터가 명확히 정해지지 않은 데이터를 뜻한다.
>   - 각 지점의 데이터는 보통 작은 단위(KB 단위)로 쪼개져 있으며 웹 사용자의 클릭 로그, 주식 정보, 사물 인터넷의 센서 데이터를 스트림 데이터로 볼 수 있다.

<br>

카파 아키텍처에서 서빙 레이어는 하둡 파일 시스템(HDFS), 오브젝트 스토리지(S3, minio 등)와 같이 데이터 플랫폼에서 흔히 사용되는 저장소이다.  
스피드 레이어로 사용되는 카프카에 분석과 프로세싱을 완료한 거대한 용량의 데이터를 오랜 기간 저장하고 사용할 수 있다면 서빙 레이어는 제거되어도 되고, 오히려 이중으로 관리되는 운영 리소스를 줄일 수 있다.  
![streaming_data_lake_architecture.jpeg](res/streaming_data_lake_architecture.jpeg)  


<br>

# 🧑🏻‍💻 카프카 실전 프로젝트
- [요구 사항](#-요구-사항)
- [적재 정책](#-적재-정책)
- [데이터 포맷](#-데이터-포맷)
- [프로듀서](#-프로듀서)
- [토픽](#-토픽)


웹 페이지의 사용자 이벤트 수집은 서비스에 영향을 미치지 않으면서도 안정적으로 유지되어야 한다.  
이벤트 수집 파이프라인과 서비스의 커플링을 최소화하고 확장성 높은 파이프라인을 만드는 것이 중요하다.  
➡ 카프카는 이러한 웹 이벤트 수집 파이프라인을 만들고 운영하기에 최적화되어 있다.

카프카는 예상치 못한 데이터의 급격한 증가가 발생하더라도 안정적으로 운영하는 데에 강점이 있다.  
웹 페이지의 사용자 이벤트가 상당히 많이 발생하더라도 발생한 이벤트들은 모두 카프카의 토픽에 쌓이기 때문이다.  
일단 토픽에 쌓이면 그 다음은 컨슈머가 자신이 할 수 있는 양만큼 최종 적재 애플리케이션에 저장하면 된다.

### ✅️ 요구 사항
이름을 입력하고 자신이 좋아하는 색상을 고르는 버튼을 누르면 해당 이벤트와 유저 에이전트 정보를 카프카 토픽으로 전달하고, 최종적으로 하둡과 엘라스틱서치에 적재되는 것을 목표로 한다.  
- 하둡
  - 대용량 데이터를 분석 처리할 수 있다.
  - HDFS: 대용량 파일을 하둡에 안정적으로 저장할 수 있게 하는 파일 시스템
  - 사용자의 이벤트를 HDFS에 적재하면 사용자 이벤트가 기하급수적으로 늘어나는 상황에서도 안정적으로 적재하고 분석할 수 있다.
- 엘라스틱서치
  - 아파치 루씬(Apache Lucene) 기반 오픈소스 분산 검색 엔진이다.
  - 엘라스틱서치에 데이터를 쌓아서 방대한 양의 데이터를 저장, 검색, 분석할 수 있다.
  - 사용자의 이벤트를 엘라스틱서치에 쌓아서 키바나를 통해 데이터를 시각화하고 분석할 수 있다.
  - 자세한 내용은 [엘라스틱서치 페이지](https://github.com/kyeoungchan/note/tree/main/elastic-stack/elasticsearch)를 참고하자.

<br>

### ✅️ 적재 정책
적재 파이프라인을 만들 때 가장 처음 결정해야하는 것은 적재 정책이다.  
파이프라인의 운영 난이도는 정책에 따라 달라진다.
- 일부 데이터가 유실되거나 중복 적재되어도 무관한 정책 ➡ 파이프라인의 운영 난이도는 낮아진다.
- 데이터가 유실, 중복 없이 정확히 한 번(exactly once) 적재되는 정책 ➡ 파이프라인 운영 난이도는 상당히 올라간다.

0.11.0.0 이후 버전부터는 멱등성 프로듀서(idempotence produce)를 통해 정확히 한 번 전달을 지원한다.  
- 여기서의 '전달': 프로듀서부터 브로커까지 전달되는 것. 적재와 별개다.
- 여기서의 '적재': 프로듀서부터 컨슈머를 넘어 최종적으로 하둡이나 엘라스틱서치까지 데이터가 저장되는 것

정확히 한 번 전달되더라도 정확히 한 번 적재는 되지 않을 수 있다.  
컨슈머의 커밋 시점과 데이터 적재가 동일 트랜잭션에서 처리되어야 정확히 한 번 적재될 수 있기 때문이다.

<br>

HDFS 적재, S3 적재 컨슈머 애플리케이션에서는 컨슈머의 커밋과 저장이 동일 트랜잭션으로 처리하는 것이 불가능하다.  
➡ HDFS 적재, S3 적재 컨슈머 애플리케이션은 이슈 발생 시점을 확인하고 특정 파티션의 특정 오프셋부터 다시 적재할 수밖에 없다.

<br>

정확히 한 번 적재가 필요할 경우 ➡ 멱등성 프로듀서를 사용하고 고유한 키를 지원하는 데이터베이스 시스템에 적재하는 것이 가장 확실하다.  
예를 들어, MySQL에 고유 키(unique key)를 가진 테이블을 생성하고 해당 테이블에 insert하는 적재 파이프라인을 만드는 경우, 컨슈머가 중복해서 insert를 시도하더라도 이미 고유 키로 적재된 데이터가 존재하므로 중복 적재되지 않게 된다.

<br>

웹 피이지의 사용자 이벤트는 사용자가 사용하는 환경(ex. 산속)에 따라 유실이 발생할 가능성이 크다.  
그리고, 최종 적재하고자 하는 HDFS 적재의 경우 트랜잭션이 지원되지 않으므로 컨슈머의 장애가 발생했을 때 데이터의 중복이 발생할 여지가 있다.  
따라서 정확히 한 번 적재 정책을 가져가는 것이 어렵기 때문에 중복도 허용할 수 있다.

<br>

파이프라인 정책
- 일부 데이터의 유실 또는 중복 허용
- 안정적으로 끊임없는 적재
- 갑작스럽게 발생하는 많은 데이터양을 허용

<br>

### ✅ 데이터 포맷
데이터 파이프라인에서 데이터를 담는 용도로 사용되는 데이터 포맷은 매우 다양한 선택지가 있다.  
우선, VO(Value Object) 형태로 객체를 선언하여 직렬화하여 전송하는 방법을 생각해볼 수 있다.  

단점
- 프로듀서와 컨슈머에서 동일한 버전의 VO 객체를 선언해서 사용해야한다는 문제점이 있다. 
- 스키마가 변경될 경우(ex. 변수가 추가되거나 삭제되는 경우) 프로듀서와 컨슈머 둘 다 소스코드 업데이트가 필요하므로 비용이 크다.
- 직렬화된 객체는 `kafka-console-consumer` 명령어를 통해 출력할 경우 내부 데이터를 확인할 수 없기 때문에 디버깅이 어려우며, 디버깅을 위해서는 해당 객체에 특화된 역직렬화 클래스가 필요하다.

<br>

데이터 포맷을 선택할 때 우선적으로 생각해볼 2가지가 있는데, 스키마의 변화 유연성과 명령어를 통한 디버깅의 편리성이다.  
➡ 2가지 모두 충족되는 데이터 포맷이 JSON(JavaScript Object Notation)이다.  
- JSON은 String으로 선언되어 토픽에 String 또는 ByteArray로 직렬화하여 전송하고, `kafka-console-consumer` 명령어를 통해 데이터를 출력할 수 있다.
- 키-값 쌍으로 이루어진 구조를 가지고 있으므로 스키마의 변경에 유연하게 대처할 수 있다.
- HDFS에 JSON 포맷으로 파일을 적재하면 아파치 하이브(Apache Hive)로 external table을 생성하여 SQL을 사용해서 필요한 정보를 쿼리로 뽑아낼 수 있다.
- elasticsearch는 JSON 포맷 기반으로 문서 파일을 저장하기 때문에 별다른 포맷 변경 없이 데이터를 적재할 수 있다.

<br>

### ✅ 프로듀서
웹 페이지에서 생성된 이벤트를 받는 REST API 클라이언트를 만들고 전달받은 이벤트를 가공하여 토픽으로 전달하는 역할을 한다.  
RestController로 받은 데이터를 토픽으로 전달할 때는 스프링 카프카 라이브러리를 사용한다.  
스프링 카프카의 `KafkaTemplate`으로 프로듀서를 구현하여 데이터를 전송한다.

<br>

**`acks` 설정**  
프로듀서를 운영할 때 가장 처음 고민해야 할 옵션은 `ack`를 어떤 값으로 설정할지다.  
- `all`: 클러스터 또는 네트워크에 이상이 생겼을 경우 복구할 확률이 가장 높지만 그만큼 프로듀서가 클러스터로 데이터를 저장하는 데에 시간이 오래걸린다.
- 1 또는 0: 속도는 빠르지만 클러스터에 이상이 생겼을 경우 데이터를 복구하지 못하고 유실이 발생할 수 있다.

이번 애플리케이션에서는 데이터 전송에 일부 유실이나 중복이 발생하더라도 안정적이고 빠른 파이프라인을 구성하는 것이 목표이므로 `acks`를 1로 설정한다.

<br>

**`min.insync.replicas` 설정**  
`acks`를 1로 설정한 경우 `min.insync.replicas` 설정을 무시하고 리더 파티션에 지속 적재하므로 따로 설정할 필요가 없다.  
➡ `acks`를 `all`로 설정했을 때만 `min.insync.replicas` 설정이 유요하다.  
참고: [카프카 프로듀서](https://github.com/kyeoungchan/note/tree/main/kafka/detail)  

<br>

**파티셔너 설정**  
파티셔너를 활용하면 메시지 키 또는 메시지 값을 기반으로 어떤 파티션으로 전달될지 결정하는 로직을 적용할 수 있다.  
그러나 웹 페이지에서 생성된 데이터는 특별히 파티션을 분류할 필요가 없기 때문에 기본 파티셔너인 `UniformStickyPartitioner`를 사용한다.  

<br>

**재시도(retries) 설정**  
클러스터 또는 네트워크 이슈로 인해 데이터가 정상적으로 전송되지 않았을 때 프로듀서는 다시 전송을 시도한다.  
프로듀서가 전송을 재시도할 경우 토픽으로 전송된 데이터의 중복이 발생할 수 있고, 전송 시점의 역전으로 인해 전송 순서와 토픽에 적재된 데이터의 순서가 바뀔 수 있다.  
여기서는 토픽의 데이터 순서를 지키지 않고 데이터의 중복을 허용하기 때문에 별도로 설정하지 않는다.

<br>

**프로듀서의 압축 옵션 설정**  
프로듀서의 압축은 `gzip`, `sanpy`, `lz4`, `zstd` 중 한 개를 고를 수 있다.  
압축을 하면 클러스터에 적재되는 데이터의 총 용량을 줄이고 네트워크의 사용량을 줄이는 데 효과적이다.  
그러나 프로듀서와 컨슈머에서 데이터를 사용할 때 CPU와 메모리 사용량이 늘어나고 압축을 하지 않았을 때보다 처리량이 줄어들 수 있다.  
여기서는 압축을 하지 않는다.

<br>

### ✅ 토픽
**파티션 개수**  
토픽을 설정할 때 가장 처음 고민하는 것은 파티션 개수다.  
파티션 개수는 데이터 처리 순서를 지켜야하는지 여부에 따라 엄격하게 정할지 말지 결정한다.  
하둡 또는 엘라스틱서치에 데이터를 순서대로 적재해야 한다면 파티션 개수를 엄격하게 정해야 한다.  

그러나 데이터를 순서대로 적재하지 않더라도 하둡과 엘라스틱서치에서 데이터를 조회할 때 시간 순서대로 조회할 수 있다.  
➡ 이벤트가 발생할 때 이벤트 발생 시간을 데이터에 같이 조합해서 보내는 것이다.  
➡ 적재되는 순서가 이벤트 발생 순서와 달라도 되기 때문에 ➡ 파티션을 여러 개로 생성하여 병렬로 컨슈머를 운영해도 된다.  
그러므로 토픽의 파티션 개수는 2개 이상으로 설정한다.

<br>

**메시지 키의 사용 여부**  
메시지 키를 사용하고 커스텀 프로듀서 파티셔너를 사용하지 않을 경우 메시지 키의 해시값으로 파티션이 분배된다.  
➡ 추후 파티션이 증가되면 해시값과 파티션의 매칭이 깨지기 때문에 메시지 키를 활용하고 특정 파티션에 할당하는 컨슈머를 운영할 경우 매우 곤란해진다.  
여기서는 메시지 키를 사용하지 않고, 토픽에 들어오는 데이터의 양에 따라 파티션 개수를 가변적으로 설정할 수 있게 한다.

<br>

**복제 개수(replication factor)**  
복제 개수가 높으면 높을수록 데이터의 복구 확률이 높아진다.  
다만, 복제 개수가 너무 높으면 팔로워 파티션이 데이터를 복제하는 데에 시간이 오래 걸릴 수 있으며, 클러스터 전체를 봤을 때 저장되는 데이터의 용량도 그만큼 더 늘어난다.  
여기서는 클러스터 브로커 1대에 이슈가 발생했을 경우에도 안정적으로 데이터를 받기 위한 최소 설정으로 2를 설정한다.

<br>

### ✅ 컨슈머
토픽에 저장되어 있는 웹 이벤드를 하둡과 엘라스틱서치에 저장하는 로직을 만드는 두 가지 방법
1. 컨슈머 API를 사용하여 직접 애플리케이션을 개발하는 방법
   - 컨슈머를 직관적으로 운영하는 가장 좋은 방법이다.
   - Java 기반 애플리케이션으로 직접 개발할 경우 타깃 애플리케이션과 연동하는 라이브러리를 직접 선택, 적용할 수 있다.
   - 컨슈머 내부 로직에 타깃 애플리케이션과 연동 시 최적화할 수 있는 옵션을 거의 제한 없이 적용할 수 있기 때문에 개발 허용 범위가 넓다.
   - 반복적으로 생성되는 파이프라인 운영이 필요할 때 확장 가능한 멀티 스레드 애플리케이션을 개발하기 위해서는 상당한 노력이 들어간다.
2. 커넥트를 사용하는 방법(참고: [카프카 커텍트](https://github.com/kyeoungchan/note/tree/main/kafka/kafkaconnect))
   - 분산 커넥트를 사용하면 REST API를 통해 커넥터로 반복적인 파이프라인을 쉽게 생성할 수 있다.
   - 커넥트를 사용할 때 오픈소스로 공개된 커텍터를 사용할 수도 있고 직접 커넥터를 개발하여 커넥트에 적용하여 사용할 수 있다.
   - 그러나 오픈소스 커넥터들은 각기 다른 라이선스를 가지고 있으며 법적인 문제로 오픈소스를 사용할 수 없을 수도 있으므로 주의해야 한다.
   - 커넥터를 직접 개발하면 법적인 문제에서 자유로울 수 있다.
     - 커넥터를 직접 개발할 경우 HDFS와 elasticsearch에 적재하기 위해서는 싱크 커넥터를 구현해야 한다.
     - 카프카 connect-api 라이브러리에 포함된 `SinkConnector`와 `SinkTask` 추상클래스를 상속받으면서 커스텀 싱크 커넥터를 개발할 수 있다.

개발하는 환경에 따라 선택하면 된다.  
이미 상용 인프라에 분산 커넥트가 구축되어 있다면 싱크 커넥터를 활용하는 것이 좋다.  
분산 커넥트가 없다면 구축하는 데에 시간과 인프라 비용이 발생할 수 있으므로 컨슈머 API를 활용하여 멀티 스레드 Java 애플리케이션을 만들고 배포하여 운영하는 것이 좋다.

여기서는 둘다 해본다.  
- HDFS 적재 ➡ 컨슈머 API를 사용하여 만든 멀티 스레드 JAVA 애플리케이션으로 개발
- elasticsearch ➡ 커스텀 싱크 커넥터를 개발하여 커넥트에 적용


<br>

**참고 자료**  
[아파치 카프카 애플리케이션 프로그래밍 with 자바](https://product.kyobobook.co.kr/detail/S000001842177)